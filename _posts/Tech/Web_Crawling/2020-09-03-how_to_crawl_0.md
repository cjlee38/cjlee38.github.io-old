---
layout: post
title:  "# Step by Step, Web crawling 학습하기 (0) - 이것 만큼은 알고 가자"
date:   2020-09-03 02:00:00 +0900
categories: web-crawling
tags: 
author: cjlee
cover: /assets/covers/web-crawling.jpg
---

# 0. 웹 크롤링이란?
: 내가 보고 있는 웹페이지는, HTML, CSS, Javascript에 의해서 이리저리 요리된 다음, 내 웹브라우저에서 보기 좋게 플레이팅 되서 나온다. 

이렇게 요리된 결과물을, 내 컴퓨터로 갖고와서, 내 입맛대로 다시 요리한 다음, 원하는대로 활용하는 것이 우리가 알고 있는 웹 크롤링이다.

그러나, **"엄밀하게는" 웹 크롤링은 위 내용과는 조금 다르다.** 이는 2번에서 다시 설명하겠다.


# 1. 크롤링, 합법인가?
: 웹 사이트를 무단으로 크롤링하는 것은 불법이다. [이 뉴스](http://news.bizwatch.co.kr/article/mobile/2017/09/27/0023/prev_ver)를 보면, '사람인' 이 '잡코리아'의 채용공고를 무단으로 크롤링하여 게재하였다가, 잡코리아로부터 소송을 당한 판례가 나타나 있다.

그렇다면, 모든 크롤링은 불법인가? 그렇지만은 않다. [이 뉴스](http://www.ddaily.co.kr/news/article/?no=151940) 에서는, 다음과 같이 이야기 하고 있다.

---

> 모든 크롤링 행위가 위법이라고 단정하기는 곤란하며, 합법적인 크롤링과 불법적인 크롤링으로 구별하는 것이 바람직하다. **합법적인 크롤링은 사이트 운영자의 의사에 반하지 않은 크롤링**을 의미하고, 반대로 불법적인 크롤링은 사이트 운영자의 의사에 반하거나 또는 실정법을 어긴 크롤링을 의미한다.
>
> 대표적으로 웹사이트 운영자가 **웹서버의 홈디렉토리에 위치한 robots.txt 파일**에 포괄적인 크롤링 금지 또는 특정 검색엔진의 크롤링 금지, 특정 디렉토리에 대한 크롤링 금지 등을 표시하였음에도 불구하고, 그 표시를 무시하고 크롤링을 하였다면 이는 사이트 운영자의 의사에 반한 크롤링에 해당하는 것이다.

---

가령, 네이버의 robots.txt를 보자 (https://www.naver.com/robots.txt)

> User-agent: *  
> Disallow: /  
> Allow : /$   

모든 user-agent에 대해서, 모든 사이트는 금지하고, 첫 페이지만 허용한다는 의미를 갖는다. 자세한 내용은 [공식 사이트](http://www.robotstxt.org/)를 참고하자.

웬만한 대형 웹사이트는 위와 같은 robots.txt를 갖고 있으므로, 크롤링을 하기 전에 이를 확인해보는 것이 좋다.

# 2. 내가 만든 프로그램, Web Crawler 인가?

: 그렇지 않다. "Crawling을 하는 프로그램이니, Cralwer 아닌가요?" 할 수 있겠지만, 엄연히 구분하자면, 내가 만든 프로그램은 Crawler가 아니다.

다음의 논문은, 웹 크롤링에 대해서 다음과 같이 정의하고 있다.

---

> 웹 크롤링(Web Crawling)이란 방대한 양의 웹페이지를 방문하고 해당 웹 페이지가 보유하고 있는 정보나 데이터 등을 자동적으로 수집하는 프로그램으로서 워싱턴 대학교의 브라이언 핑커튼(Brian Pinkerton)에 의해 개발되었다. 웹 크롤링의 수행 과정은 크게 2가지 단계로 나타나는데, 첫 번째로 웹 사이트의 주소와 그 주소에 대한 HTML 태그 전체를 불러오는 크롤링(Crawling) 단계와 두 번째로 크롤링 된 HTML 태그 내에서 원하는 데이터를 추출하는 작업인 스크래핑(Scraping) 단계이다.  

> Source : 김성우, 강동현. (2019). 실제 환경과 가상머신 환경에서의 웹 크롤링 성능 비교. 한국정보과학회 학술발표논문집, (), 2067-2069.
{: .caption}

---

구글과 네이버같이 검색엔진을 보유하고 있는 웹사이트들은, 저마다의 "Crawler"를 갖고 있다. 이 "Crawler"는, 각종 웹사이트를 돌면서, 그 내부에 있는 URL을 찾아가면서, 계속해서 URL과 그 내용을 저장한다. 그리고 이렇게 저장된 내용을 우리가 "검색" 했을 때, 잘 정리정돈해서 보여주는 것이고, 그 과정 속에서 SEO 전략 등이 고안되는 것이다.

---
> Note. 인터넷을 많이 사용해본 사람이라면, 이런 경험을 해본 적이 있을 것이다.
>  예를 들어, 스타크래프트 라는 키워드로 검색을 했는데, 어떤 블로그에서   
> "스타크래프트 ~ 버전 ... 다운로드 만두를 먹고싶은 사람이 ... 어제 선풍기를 틀고 잤더니 감기에 걸렸다"
> 이런 정신나간 듯한 글이 여러 개가 작성되어 있는 것이다. 이런 것들이, 예~전 웹 크롤러의 SEO 알고리즘에 대응하여, 단순 조회수, 방문수를 늘리기 위하여 작성된 글이다.

---

따라서, 우리가 만든 프로그램은 Crawler 라고 볼 수 없다. 단순히 http get request를 통해 얻은 결과물을 우리 입맛대로 조리할 뿐이다. 

좀 더 자세하게 알고 싶다면 [여기](https://velog.io/@mowinckel/%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%81-I)를 참고해보자. 약간의 비속어가 섞여 있긴 하지만, 내 개인적인 생각과 일맥상통한다. 

# 3. 따라서
: 우리가 만들고자 하는 프로그램은, 정확히는 Crawler라고 보기는 어려우며, 단순히 "데이터를 수집하는 프로그램" 이라고 칭할 수 있겠다. 

따라서, Web crawling이라고 칭하고, 이에 대해서 작성하겠지만, 정확히는 단순 http request이다.

또한, 이는 단발성으로 "과제"를 해결하는 용도로, 합법적인 선 내에서 수행하길 바라며, 이에 대한 법적인 책임은 본인에게 있다는 사실을 명심하자.


---
layout: post
title:  "# Step by Step, Web crawling 학습하기 (2) - Request"
date:   2021-01-07 19:08:00 +0900
categories: web-crawling
tags: 
author: cjlee
cover: /assets/covers/web-crawling.jpg
---

# 0. 들어가며
: [지난 포스팅](https://cjlee38.github.io/web-crawling/how_to_crawl_1) 에 이어, 이번에는 본격적으로 데이터를 수집해보는 연습을 해보자.

데이터를 수집하고자 하는 방법과 라이브러리는 크게 두 가지로 구분할 수 있다.

* 직접 Http 요청 보내기 - requests
* 웹 브라우저를 자동화하기 - Selenium

대부분의 웹사이트는 이 두 가지 라이브러리를 사용(혹은 혼용)함으로써 원하는 html을 얻어낼 수 있고, 이를 BeautifulSoup 라이브러리를 이용해 파싱하는 것이 일반적이다.

# 1. 수집 과정에 대한 이해
: 위와 같이 Http 요청을 보내는 방법도 있고, 웹 브라우저를 이용하는 방법이 있다면, 그 방법들의 차이점은 무엇일까? 

> 기존에 웹이 어떻게 동작하는 것인지에 대해서 이해하고 있다면, 스킵해도 무방하다.

### 1) 수동으로 데이터를 수집

: 우리가 일반적으로 생각하는, **사람이 직접** 데이터를 수집하는 방법은 아래 그림과 같다.

![](/assets/images/2021-01-07-19-21-11_2021-01-07-how_to_crawl_2.md.png)

웹 브라우저를 이용해서 어떤 사이트에 접속해서, 직접 복사 붙여넣기를 하거나, 필요한 부분만 눈으로 보면서 엑셀이나 메모장 등에 직접 타이핑하는 등의 수고를 거친다.

그러나 그 이면에서 어떻게 동작하는 것인지에 대해서 이해가고 있어야 한다.

![](/assets/images/2021-01-07-19-28-48_2021-01-07-how_to_crawl_2.md.png)

우리가 웹브라우저를 통해 네이버 홈페이지에 접속했다고 해보자. 그러면, 해당 주소에 대한 정보를 갖고있는 네이버 서버가, 그 요청을 받고, 그에 걸맞는 html, css, javascript 등의 정보를 돌려준다. 이렇게 받아낸 정보를 웹브라우저가 잘 해석해서, 예쁘게 접시에 담아 우리에게 보여주는 것이다.

그렇다면 Requests 를 이용하는 방법과 Selenium 을 이용하는 방법은, 위와 같은 과정에, 코드를 첨가해서 자동화를 이끌어낸다.

### 2) 자동으로 데이터를 수집 - Requests

먼저 Requests를 이용하는 방법이다.

![](/assets/images/2021-01-07-19-35-23_2021-01-07-how_to_crawl_2.md.png)

Requests를 이용했을 때에는, 위와 같이 웹 브라우저를 거치지 않고, 네이버 서버로 곧바로 요청을 보낸다. 요청을 받은 네이버는 마찬가지로 우리에게 데이터를 돌려준다. 

이 데이터를 우리는 파이썬에서 BeautifulSoup를 이용해 다루기 쉬운 객체로 만든 다음, 코드를 이용해 필요한 데이터의 위치를 찾아서, 우리가 원하는 데이터의 형태로 재구성하게 된다.

### 3) 자동으로 데이터 수집 - Selenium

![](/assets/images/2021-01-07-19-43-34_2021-01-07-how_to_crawl_2.md.png)

Selenium 을 이용하는 방법은, 위 두 가지 방법을 합친것과 비슷하다. 사람이 직접 웹 브라우저에 주소창에 주소를 쳐서 접속하는 과정을 자동화한다.

그리고, 마찬가지로 BeautifulSoup를 이용해 다루기 쉬운 객체로 만들수도 있고, 혹은 Selenium에서 직접 다룰수도 있다. 이 내용은 차차 알아가자.

---

여기까지의 배경지식을 쌓았다면, 실제로 requests를 이용해 데이터를 수집하는 과정을 경험해보자.

데이터를 수집할 대상은, `robots.txt` 를 확인해 보았을 때 문제가 없으면서, 나름대로의 구조화가 되어있는 [프로그래머스의 모든 문제](https://programmers.co.kr/learn/challenges?tab=all_challenges) 라는 페이지 이다.

![](/assets/images/2021-01-30-03-23-14_2021-01-07-how_to_crawl_2.md.png)

당장 눈앞에 보이는 한 페이지의 문제 제목들을 가져와보자.

```python
import requests

url = "https://programmers.co.kr/learn/challenges?tab=all_challenges"
resp = requests.get(url)

print(resp.status_code)
# 200
```

이는 위 url 을 대상으로 "GET" 이라는 메소드를 전달하겠다는 뜻이며, 이를 전달받은 프로그래머스 서버는 이에 기반한 정보를 우리에게 돌려주고, 이를 resp 라는 이름의 변수에 저장한다.

곧바로 다음에 위치한 status_code는 요청을 보냈을 때, 정상적으로 처리가 되었는지, 그렇지 않다면 어떤 문제가 발생하였는지에 대한 정보를 제공해주는 HTTP 상태 코드를 의미한다. 자세한 내용은 [여기](https://developer.mozilla.org/ko/docs/Web/HTTP/Status) 에서 확인하자.

요청 결과가 정상적으로 처리되었음을 알리는 200 코드를 확인했다면, 다음을 입력해보자.

```python
print(resp.text)
```

![](/assets/images/2021-01-30-03-37-17_2021-01-07-how_to_crawl_2.md.png)

알아보기 힘든 갑작스러운 글자들이 눈을 어지럽히는데, 전혀 당황할 필요가 없다. 이는 정상이다. 그리고, 조금만 집중해서 살펴보면, 한글로 써있는 부분은 `코딩테스트 연습` 과 같이, 뭔가 익숙한 부분이 보이기도 한다.

지금 우리가 받은 이 내용은, 위에서 언급한 HTML 을 받아온 것이다. 아직 접시에 예쁘게 담기기 전인, **냄비에 들어 있는 상태 정도**로 이해하면 될 듯 싶다.

직접 웹브라우저로 해당 페이지로 이동해서, `F12` 버튼을 눌러보면, Elements 라는 탭에서 비슷한 코드를 볼 수 있다. 자세히 비교해보면, 앞서 requests 모듈을 통해 얻어낸 데이터와 일치하는 것을 확인 할 수 있을 것이다.

---

> 정확히는, request 의 headers, params 등으로 인해 차이가 있을 수 있다.

---

다음으로 해야할 일은, 접시에 예쁘게 담아내지는 못하더라도, 식사를 할 수 있을 정도로 용이하도록 그릇에 쏟아부어야 한다.

이는 역시, 앞에서 언급한 `BeautifulSoup` 라는 라이브러리를 이용하면, 간편하게 처리할 수 있다.

```python
from bs4 import BeautifulSoup as bs

soup = bs(resp.text, 'lxml')
print(soup)
```

![](/assets/images/2021-01-30-03-37-49_2021-01-07-how_to_crawl_2.md.png)

내용 자체에는 변화가 없지만, 아까와는 다르게 조금 정렬된 느낌이 든다.  
앞선 `resp.text` 자체는 문자열이었지만, 현재의 soup 라는 녀석은 BeautifulSoup 라는 객체이다. 그리고 우리는 이 녀석을 통해, **쉽게 데이터를 조작할 수 있다.**

다시 웹브라우저로 돌아와서, `Ctrl + Shift + C` 를 누르고 문제의 이름에 마우스를 올려보면, 해당하는 녀석이 HTML 에서 어떻게 구성되어 있는지 가늠해볼 수 있다.

![](/assets/images/2021-01-30-03-42-39_2021-01-07-how_to_crawl_2.md.png)

위 사진을 보면, `title` 이라는 `class(속성)`를 갖고 있는, `h4` 태그 안에 `멀쩡한 사각형` 이라는 텍스트가 있다.

![](/assets/images/2021-01-30-03-45-59_2021-01-07-how_to_crawl_2.md.png)

마찬가지로 `Ctrl + Shift + C` 를 통해 다른 문제에 마우스를 올려봤을 때, `프린터` 라는 녀석 또한 `title` 이라는 `class`를 갖고 있는, `h4` 태그 안에 있음을 확인할 수 있다.

---

이렇게 패턴을 발견했다면, 다음 할 일은 간단해진다.

**`h4` 태그 이면서, 동시에 `title` 이라는 `class`를 가진 녀석이 있나요?** 라고 BeautifulSoup 객체에게 물어보면 된다.

```python
probs = soup.find_all("h4", class_ = "title")
print(probs)

# [<h4 class="title">모든 레코드 조회하기</h4>,
#  <h4 class="title">두 개 뽑아서 더하기</h4>,
#  <h4 class="title">크레인 인형뽑기 게임</h4>,
#  <h4 class="title">최댓값 구하기</h4>,
#  <h4 class="title">스킬트리</h4>,
#  <h4 class="title">프린터</h4>,
#  <h4 class="title">다리를 지나는 트럭</h4>,
#  <h4 class="title">기능개발</h4>,
#  <h4 class="title">주식가격</h4>,
#  <h4 class="title">멀쩡한 사각형</h4>,
#  <h4 class="title">124 나라의 숫자</h4>,
#  <h4 class="title">[1차] 추석 트래픽</h4>,
#  <h4 class="title">지형 이동</h4>,
#  <h4 class="title">완주하지 못한 선수</h4>,
#  <h4 class="title">신규 아이디 추천</h4>,
#  <h4 class="title">카카오프렌즈 컬러링북</h4>,
#  <h4 class="title">문자열 압축</h4>,
#  <h4 class="title">삼각 달팽이</h4>,
#  <h4 class="title">N으로 표현</h4>,
#  <h4 class="title">2 x n 타일링</h4>]
```

운이 좋게도, 문제 제목에 해당하는 녀석들 이외에는, `h4` 태그이면서 동시에 `class` 가 `title` 인 녀석은 없었다.

그리고, 이 녀석은 아직도 BeautifulSoup 객체이므로, 그 안에 들어있는 텍스트, 즉 문자열을 꺼내오면 된다.

```python
titles = []
for prob in probs :
    titles.append(prob.get_text())
print(titles)

# ['모든 레코드 조회하기', '두 개 뽑아서 더하기', '크레인 인형뽑기 게임', '최댓값 구하기', '스킬트리', '프린터', '다리를 지나는 트럭', '기능개발', '주식가격', '멀쩡한 사각형', '124 나라의 숫자', '[1차] 추석 트래픽', '지형 이동', '완주하지 못한 선수', '신규 아이디 추천', '카카오프렌즈 컬러링북', '문자열 압축', '삼각 달팽이', 'N으로 표현', '2 x n 타일링']
```

probs 안에 있는 녀석들로부터, 하나씩 꺼내서 `get_text()` 라는 함수를 호출해서 문자열을 꺼내고, 이를 titles 라는 녀석에 저장함으로서 데이터를 수집하였다.

여러 페이지에 대해서 이렇게 진행이 가능하다면, 프로그래머스에 있는 모든 문제들에 대한 정보를 가져올 수 있을 것이다.

그러나, **아쉽게도 requests 모듈로 이를 다루기에는 쉽지 않다.** 웹에 대한 꽤나 깊은 이해가 필요하기 때문에, 속도가 중요하지 않다면 차선책으로 Selenium 을 선택한다.

다음 포스팅에서 **Selenium**의 사용법에 대해 알아보면서, **어떤 상황에 어떤 라이브러리를 선택하는 것이 좋은 것인지**까지 알아보자.

이상으로 포스팅을 마칩니다.